{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:17:38.471573286Z",
     "start_time": "2023-06-17T12:17:36.255927662Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 14:17:36.473023: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-17 14:17:36.594846: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-17 14:17:36.595543: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-17 14:17:37.332206: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import gpflow\n",
    "import tensorflow as tf\n",
    "from gpflow.kernels.stationaries import SquaredExponential, Kernel, Matern52\n",
    "from tensorflow import Tensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:17:38.475236630Z",
     "start_time": "2023-06-17T12:17:38.474169499Z"
    }
   },
   "outputs": [],
   "source": [
    "# old stuff\n",
    "\n",
    "def old_k_grad_se(inducing_variable_ny, inducing_variable_Zy, kernel):\n",
    "    funcs2 = tf.range(inducing_variable_Zy.shape[0])\n",
    "    norm = 1 / (kernel.lengthscales ** 2)\n",
    "    partial_derivative = tf.map_fn(\n",
    "        lambda j: (inducing_variable_ny - inducing_variable_Zy[j]),\n",
    "        funcs2, fn_output_signature=tf.float64, parallel_iterations=8)\n",
    "    return norm * partial_derivative\n",
    "\n",
    "def old_kernel_derivative(\n",
    "        inducing_location_ny: Tensor,\n",
    "        inducing_location_Zy: Tensor,\n",
    "        kernel: Kernel,\n",
    "):\n",
    "    iterator = tf.range(inducing_location_ny.shape[0])\n",
    "    block = tf.map_fn(lambda i: old_k_grad_se(inducing_location_ny[i], inducing_location_Zy, kernel),\n",
    "                      iterator, fn_output_signature=tf.float64, parallel_iterations=8)\n",
    "\n",
    "    return block\n",
    "\n",
    "def initialize_Z(num_latent_gps, num_inducing):\n",
    "    Z = tf.convert_to_tensor(np.array(\n",
    "        [np.full(num_latent_gps, i) for i in np.linspace(0.1, 0.9, num_inducing)], dtype=np.float64))\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:17:38.634902412Z",
     "start_time": "2023-06-17T12:17:38.476348403Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 14:17:38.531675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-17 14:17:38.608651: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "dof = 1\n",
    "from gpflow.base import default_float\n",
    "\n",
    "start = tf.cast((tf.fill((1, dof), 0.)), dtype=default_float())\n",
    "end = tf.cast((tf.fill((1, dof), 1.)), dtype=default_float())\n",
    "Z = initialize_Z(dof, 2)\n",
    "ny = tf.concat([start, end], axis=0)\n",
    "Zy = tf.concat([ny, Z], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:17:38.641376782Z",
     "start_time": "2023-06-17T12:17:38.637173217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([4, 1])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:17:38.745978813Z",
     "start_time": "2023-06-17T12:17:38.641221029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Setting parallel_iterations > 1 has no effect when executing eagerly. Consider calling map_fn with tf.function to execute fn in parallel.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 4, 1), dtype=float64, numpy=\narray([[[ 0. ],\n        [-1. ],\n        [-0.1],\n        [-0.9]],\n\n       [[ 1. ],\n        [ 0. ],\n        [ 0.9],\n        [ 0.1]]])>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_kernel_derivative(ny, Zy, SquaredExponential())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:17:38.748937073Z",
     "start_time": "2023-06-17T12:17:38.746390371Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_kernel_derivative(ny, Zy, kernel):\n",
    "    lengthscale = kernel.lengthscales\n",
    "    block = new_k_grad_se(ny, Zy, lengthscale)\n",
    "    return block\n",
    "\n",
    "def new_k_grad_se(ny, Zy, lengthscale):\n",
    "    norm = 1 / (lengthscale ** 2)\n",
    "    partial_derivative = norm * tf.subtract(tf.expand_dims(ny, 1), Zy)\n",
    "    return partial_derivative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:17:38.792782315Z",
     "start_time": "2023-06-17T12:17:38.749945794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 4, 1), dtype=float64, numpy=\narray([[[ 0. ],\n        [-1. ],\n        [-0.1],\n        [-0.9]],\n\n       [[ 1. ],\n        [ 0. ],\n        [ 0.9],\n        [ 0.1]]])>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_kernel_derivative(ny, Zy, SquaredExponential())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:17:38.793294866Z",
     "start_time": "2023-06-17T12:17:38.792603272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4, 4, 1), dtype=float64, numpy=\narray([[[ 0. ],\n        [-1. ],\n        [-0.1],\n        [-0.9]],\n\n       [[ 1. ],\n        [ 0. ],\n        [ 0.9],\n        [ 0.1]],\n\n       [[ 0.1],\n        [-0.9],\n        [ 0. ],\n        [-0.8]],\n\n       [[ 0.9],\n        [-0.1],\n        [ 0.8],\n        [ 0. ]]])>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_kernel_derivative(Zy, Zy, SquaredExponential())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:17:38.793490950Z",
     "start_time": "2023-06-17T12:17:38.793065671Z"
    }
   },
   "outputs": [],
   "source": [
    "def k_grad_grad_se_fallback(\n",
    "        inducing_location_ny: Tensor,\n",
    "        kernel: SquaredExponential,\n",
    "):\n",
    "    norm = kernel.lengthscales ** 2\n",
    "    inducing_diff = tf.expand_dims(inducing_location_ny, 1) - tf.expand_dims(inducing_location_ny, 0)\n",
    "    second_derivative = norm - inducing_diff ** 2\n",
    "    return second_derivative / kernel.lengthscales ** 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:17:38.793652293Z",
     "start_time": "2023-06-17T12:17:38.793421288Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def kernel_derivative(ny, Zy, kernel):\n",
    "\n",
    "    block_original = kernel(Zy)[..., None]\n",
    "    print(block_original.shape)\n",
    "    ny_block = k_grad_grad_se_fallback(ny, kernel) * block_original[:ny.shape[0], :ny.shape[0]]\n",
    "    print(ny_block.shape)\n",
    "    nyZy_block = new_kernel_derivative(ny, Zy[ny.shape[0]:], kernel) * block_original[:ny.shape[0], ny.shape[0]:]\n",
    "    print(nyZy_block.shape)\n",
    "    upper_block = tf.concat([ny_block, nyZy_block], axis=1)\n",
    "    lower_block = tf.concat([-tf.transpose(nyZy_block, perm=(1, 0, 2)), block_original[ny.shape[0]:, ny.shape[0]:]], axis=1)\n",
    "    return tf.concat([upper_block, lower_block], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:17:39.309538461Z",
     "start_time": "2023-06-17T12:17:38.793580382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 1)\n",
      "(2, 2, 1)\n",
      "(2, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "aux = kernel_derivative(ny, Zy, SquaredExponential(lengthscales=.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:17:39.310127943Z",
     "start_time": "2023-06-17T12:17:39.266970985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 4, 4), dtype=float64, numpy=\narray([[[ 4.        , -1.6240234 , -0.39207947, -0.71243532],\n        [-1.6240234 ,  4.        ,  0.71243532,  0.39207947],\n        [ 0.39207947, -0.71243532,  1.        ,  0.2780373 ],\n        [ 0.71243532, -0.39207947,  0.2780373 ,  1.        ]]])>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(aux, perm=(2, 0 ,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:17:39.310371468Z",
     "start_time": "2023-06-17T12:17:39.309797166Z"
    }
   },
   "outputs": [],
   "source": [
    "kk = SquaredExponential(lengthscales=.5)\n",
    "# tf.print(kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:17:39.310773405Z",
     "start_time": "2023-06-17T12:17:39.309984480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[0.],\n       [1.]])>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:17:39.311150653Z",
     "start_time": "2023-06-17T12:17:39.310269246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2), dtype=float64, numpy=\narray([[-4.        , -0.54134113],\n       [-0.54134113, -4.        ]])>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ny[0] - ny[1])/(0.5 ** 2) * kk(ny) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:17:39.311531781Z",
     "start_time": "2023-06-17T12:17:39.310523261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 2), dtype=float64, numpy=\narray([[[ 0.        ,  0.        ],\n        [-0.54134113, -4.        ]],\n\n       [[ 4.        ,  0.54134113],\n        [ 0.        ,  0.        ]]])>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_kernel_derivative(ny, ny, kk) * kk(ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [0.062501 0.05859475 1e-06 -0.062499 -0.006249 -0.0084229130434782615 -0.010596826086956525 -0.012770739130434786 -0.014944652173913045 -0.0171185652173913 -0.019292478260869564 -0.02146639130434783 -0.023640304347826088 -0.02581421739130435 -0.027988130434782606 -0.030162043478260864 -0.03233595652173913 -0.034509869565217392 -0.036683782608695654 -0.03885769565217391 -0.041031608695652172 -0.043205521739130434 -0.045379434782608696 -0.047553347826086952 -0.049727260869565214 -0.051901173913043476 -0.054075086956521738 -0.056249]\n",
    "  [0.05859475 0.062501 0.062501 1e-06 0.056251 0.05407708695652174 0.051903173913043478 0.049729260869565216 0.047555347826086961 0.0453814347826087 0.043207521739130436 0.041033608695652174 0.038859695652173912 0.03668578260869565 0.034511869565217394 0.032337956521739139 0.03016404347826087 0.027990130434782608 0.025816217391304346 0.02364230434782609 0.021468391304347828 0.019294478260869566 0.017120565217391304 0.014946652173913047 0.012772739130434784 0.010598826086956522 0.00842491304347826 0.0062509999999999987]\n",
    "  [1e-06 -0.062499 0.500001 0.40176228684453036 0.49890844541724522 0.49801799801789431 0.49686607930323784 0.49545451174916094 0.49378552477394472 0.49186174887081535 0.489686208699424 0.48726231516165164 0.48459385649079978 0.48168498838674756 0.478540223233031 0.4751644184350019 0.47156276392124319 0.46774076885324445 0.46370424759095413 0.45945930496422421 0.45501232090233013 0.45036993447567875 0.44553902740550355 0.44052670709878128 0.43534028926678386 0.42998728018660409 0.4244753586656535 0.41881235776953657]\n",
    "  [0.062501 1e-06 0.40176228684453036 0.500001 0.41881235776953657 0.4244753586656535 0.42998728018660409 0.43534028926678381 0.44052670709878128 0.44553902740550355 0.45036993447567875 0.45501232090233013 0.45945930496422421 0.46370424759095413 0.46774076885324445 0.47156276392124319 0.47516441843500185 0.47854022323303108 0.48168498838674756 0.48459385649079983 0.48726231516165164 0.48968620869942392 0.49186174887081541 0.49378552477394472 0.495454511749161 0.49686607930323784 0.49801799801789431 0.49890844541724522]\n",
    "  [0.0062510000000000005 -0.056249 0.49890844541724522 0.41881235776953657 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569 0.49525991588553137 0.4935589282851221 0.49160350843613565 0.48939672973238346 0.48694205074937669 0.48424330621240924 0.48130469701492223 0.47813077932351289 0.47472645280913361 0.47109694804702379 0.4672478131307139 0.46318489954803271 0.45891434736941544 0.45444256980095132 0.44977623715650628 0.44492226030491427 0.43988777364963222 0.4346801176994029]\n",
    "  [0.00842491304347826 -0.054075086956521738 0.49801799801789431 0.4244753586656535 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569 0.49525991588553137 0.4935589282851221 0.49160350843613565 0.48939672973238346 0.48694205074937669 0.48424330621240924 0.48130469701492223 0.47813077932351289 0.47472645280913361 0.47109694804702379 0.4672478131307139 0.46318489954803266 0.45891434736941544 0.45444256980095132 0.44977623715650628 0.44492226030491427 0.43988777364963222]\n",
    "  [0.010598826086956524 -0.051901173913043476 0.49686607930323784 0.42998728018660409 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569 0.49525991588553137 0.4935589282851221 0.49160350843613565 0.48939672973238352 0.48694205074937669 0.48424330621240924 0.48130469701492223 0.47813077932351289 0.47472645280913361 0.47109694804702379 0.4672478131307139 0.46318489954803271 0.45891434736941544 0.45444256980095132 0.44977623715650628 0.44492226030491427]\n",
    "  [0.012772739130434784 -0.049727260869565214 0.49545451174916094 0.43534028926678381 0.49881149090400784 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569 0.49525991588553137 0.4935589282851221 0.49160350843613565 0.48939672973238346 0.48694205074937669 0.48424330621240924 0.48130469701492229 0.47813077932351289 0.47472645280913361 0.47109694804702379 0.4672478131307139 0.46318489954803271 0.45891434736941544 0.45444256980095132 0.44977623715650628]\n",
    "  [0.014946652173913043 -0.047553347826086959 0.49378552477394472 0.44052670709878128 0.49788827395673624 0.49881149090400784 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569 0.49525991588553137 0.4935589282851221 0.49160350843613565 0.48939672973238346 0.48694205074937669 0.48424330621240924 0.48130469701492223 0.47813077932351289 0.47472645280913361 0.47109694804702379 0.4672478131307139 0.46318489954803271 0.45891434736941544 0.45444256980095132]\n",
    "  [0.017120565217391304 -0.045379434782608696 0.49186174887081535 0.44553902740550355 0.49670379107443569 0.49788827395673624 0.49881149090400784 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569 0.49525991588553137 0.4935589282851221 0.49160350843613565 0.48939672973238346 0.48694205074937669 0.48424330621240924 0.48130469701492223 0.47813077932351283 0.47472645280913361 0.47109694804702379 0.4672478131307139 0.46318489954803266 0.45891434736941544]\n",
    "  [0.019294478260869566 -0.043205521739130434 0.489686208699424 0.45036993447567875 0.49525991588553137 0.49670379107443569 0.49788827395673624 0.49881149090400784 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569 0.49525991588553137 0.4935589282851221 0.49160350843613565 0.48939672973238346 0.48694205074937669 0.48424330621240924 0.48130469701492223 0.47813077932351289 0.47472645280913361 0.47109694804702379 0.46724781313071395 0.46318489954803271]\n",
    "  [0.021468391304347832 -0.041031608695652172 0.48726231516165164 0.45501232090233013 0.4935589282851221 0.49525991588553137 0.49670379107443569 0.49788827395673624 0.49881149090400784 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569 0.49525991588553137 0.4935589282851221 0.49160350843613565 0.48939672973238352 0.48694205074937669 0.48424330621240924 0.48130469701492229 0.47813077932351289 0.47472645280913361 0.47109694804702379 0.46724781313071395]\n",
    "  [0.02364230434782609 -0.03885769565217391 0.48459385649079978 0.45945930496422421 0.49160350843613565 0.4935589282851221 0.49525991588553137 0.49670379107443569 0.49788827395673624 0.49881149090400784 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569 0.49525991588553137 0.4935589282851221 0.49160350843613565 0.48939672973238352 0.48694205074937669 0.48424330621240924 0.48130469701492223 0.47813077932351289 0.47472645280913361 0.47109694804702379]\n",
    "  [0.025816217391304352 -0.036683782608695648 0.48168498838674756 0.46370424759095413 0.48939672973238346 0.49160350843613565 0.4935589282851221 0.49525991588553137 0.49670379107443569 0.49788827395673624 0.49881149090400784 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569 0.49525991588553137 0.4935589282851221 0.49160350843613565 0.48939672973238346 0.48694205074937669 0.48424330621240924 0.48130469701492229 0.47813077932351289 0.47472645280913361]\n",
    "  [0.027990130434782608 -0.034509869565217392 0.478540223233031 0.46774076885324445 0.48694205074937669 0.48939672973238346 0.49160350843613565 0.4935589282851221 0.49525991588553137 0.49670379107443569 0.49788827395673624 0.49881149090400784 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569 0.49525991588553131 0.4935589282851221 0.49160350843613565 0.48939672973238346 0.48694205074937669 0.48424330621240924 0.48130469701492223 0.47813077932351283]\n",
    "  [0.030164043478260866 -0.032335956521739137 0.4751644184350019 0.47156276392124319 0.48424330621240924 0.48694205074937669 0.48939672973238352 0.49160350843613565 0.4935589282851221 0.49525991588553137 0.49670379107443569 0.49788827395673624 0.49881149090400784 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569 0.49525991588553137 0.4935589282851221 0.49160350843613565 0.48939672973238346 0.48694205074937669 0.48424330621240924 0.48130469701492223]\n",
    "  [0.032337956521739132 -0.030162043478260868 0.47156276392124319 0.47516441843500185 0.48130469701492223 0.48424330621240924 0.48694205074937669 0.48939672973238346 0.49160350843613565 0.4935589282851221 0.49525991588553137 0.49670379107443569 0.49788827395673624 0.49881149090400784 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569 0.49525991588553137 0.4935589282851221 0.49160350843613565 0.48939672973238346 0.48694205074937669 0.48424330621240924]\n",
    "  [0.034511869565217394 -0.027988130434782606 0.46774076885324445 0.47854022323303108 0.47813077932351289 0.48130469701492223 0.48424330621240924 0.48694205074937669 0.48939672973238346 0.49160350843613565 0.4935589282851221 0.49525991588553137 0.49670379107443569 0.49788827395673624 0.49881149090400784 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569 0.49525991588553137 0.4935589282851221 0.49160350843613565 0.48939672973238346 0.48694205074937669]\n",
    "  [0.036685782608695656 -0.025814217391304344 0.46370424759095413 0.48168498838674756 0.47472645280913361 0.47813077932351289 0.48130469701492223 0.48424330621240924 0.48694205074937669 0.48939672973238346 0.49160350843613565 0.4935589282851221 0.49525991588553137 0.49670379107443569 0.49788827395673624 0.49881149090400784 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569 0.49525991588553137 0.4935589282851221 0.49160350843613565 0.48939672973238352]\n",
    "  [0.038859695652173912 -0.023640304347826088 0.45945930496422421 0.48459385649079983 0.47109694804702379 0.47472645280913361 0.47813077932351289 0.48130469701492229 0.48424330621240924 0.48694205074937669 0.48939672973238346 0.49160350843613565 0.4935589282851221 0.49525991588553137 0.49670379107443569 0.49788827395673624 0.49881149090400784 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569 0.49525991588553131 0.4935589282851221 0.49160350843613565]\n",
    "  [0.041033608695652174 -0.021466391304347826 0.45501232090233013 0.48726231516165164 0.4672478131307139 0.47109694804702379 0.47472645280913361 0.47813077932351289 0.48130469701492223 0.48424330621240924 0.48694205074937669 0.48939672973238352 0.49160350843613565 0.4935589282851221 0.49525991588553131 0.49670379107443569 0.49788827395673624 0.49881149090400784 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400778 0.49788827395673624 0.49670379107443569 0.49525991588553137 0.4935589282851221]\n",
    "  [0.043207521739130436 -0.019292478260869564 0.45036993447567875 0.48968620869942392 0.46318489954803271 0.4672478131307139 0.47109694804702379 0.47472645280913361 0.47813077932351289 0.48130469701492223 0.48424330621240924 0.48694205074937669 0.48939672973238352 0.49160350843613565 0.4935589282851221 0.49525991588553137 0.49670379107443569 0.49788827395673624 0.49881149090400784 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569 0.49525991588553137]\n",
    "  [0.0453814347826087 -0.0171185652173913 0.44553902740550355 0.49186174887081541 0.45891434736941544 0.46318489954803266 0.4672478131307139 0.47109694804702379 0.47472645280913361 0.47813077932351283 0.48130469701492223 0.48424330621240924 0.48694205074937669 0.48939672973238346 0.49160350843613565 0.4935589282851221 0.49525991588553137 0.49670379107443569 0.49788827395673624 0.49881149090400784 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400784 0.49788827395673624 0.49670379107443569]\n",
    "  [0.047555347826086954 -0.014944652173913048 0.44052670709878128 0.49378552477394472 0.45444256980095132 0.45891434736941544 0.46318489954803271 0.4672478131307139 0.47109694804702379 0.47472645280913361 0.47813077932351289 0.48130469701492229 0.48424330621240924 0.48694205074937669 0.48939672973238346 0.49160350843613565 0.4935589282851221 0.49525991588553137 0.49670379107443569 0.49788827395673624 0.49881149090400778 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662163 0.49947197949314981 0.49881149090400778 0.49788827395673624]\n",
    "  [0.049729260869565216 -0.012770739130434786 0.43534028926678386 0.495454511749161 0.44977623715650628 0.45444256980095132 0.45891434736941544 0.46318489954803271 0.4672478131307139 0.47109694804702379 0.47472645280913361 0.47813077932351289 0.48130469701492223 0.48424330621240924 0.48694205074937669 0.48939672973238346 0.49160350843613565 0.4935589282851221 0.49525991588553137 0.49670379107443569 0.49788827395673624 0.49881149090400784 0.49947197949314981 0.49986869236662163 0.500001 0.49986869236662157 0.49947197949314975 0.49881149090400778]\n",
    "  [0.051903173913043478 -0.010596826086956524 0.42998728018660409 0.49686607930323784 0.44492226030491427 0.44977623715650628 0.45444256980095132 0.45891434736941544 0.46318489954803271 0.4672478131307139 0.47109694804702379 0.47472645280913361 0.47813077932351289 0.48130469701492229 0.48424330621240924 0.48694205074937669 0.48939672973238346 0.49160350843613565 0.4935589282851221 0.49525991588553131 0.49670379107443569 0.49788827395673624 0.49881149090400784 0.49947197949314981 0.49986869236662157 0.500001 0.49986869236662157 0.49947197949314981]\n",
    "  [0.05407708695652174 -0.0084229130434782615 0.4244753586656535 0.49801799801789431 0.43988777364963222 0.44492226030491427 0.44977623715650628 0.45444256980095132 0.45891434736941544 0.46318489954803266 0.46724781313071395 0.47109694804702379 0.47472645280913361 0.47813077932351289 0.48130469701492223 0.48424330621240924 0.48694205074937669 0.48939672973238346 0.49160350843613565 0.4935589282851221 0.49525991588553137 0.49670379107443569 0.49788827395673624 0.49881149090400778 0.49947197949314975 0.49986869236662157 0.500001 0.49986869236662163]\n",
    "  [0.056251 -0.0062489999999999985 0.41881235776953657 0.49890844541724522 0.4346801176994029 0.43988777364963222 0.44492226030491427 0.44977623715650628 0.45444256980095132 0.45891434736941544 0.46318489954803271 0.46724781313071395 0.47109694804702379 0.47472645280913361 0.47813077932351283 0.48130469701492223 0.48424330621240924 0.48694205074937669 0.48939672973238352 0.49160350843613565 0.4935589282851221 0.49525991588553137 0.49670379107443569 0.49788827395673624 0.49881149090400778 0.49947197949314981 0.49986869236662163 0.500001]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vgpmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f01e6a756d1939fec269a02bc8944db8b372d9c47942ed0978bffe99fee7403"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
